{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the network of this notebook we obtained a result of 72,4% on the test set\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_anode_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and we set the seed in order to make the experiment replicable\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)  \n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if you are using Colab with Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip \"/content/drive/My Drive/data.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGenerator: we apply some data augmentation in order to somehow face the scarsity of the training set\n",
    "# ------------------\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "apply_data_augmentation = True\n",
    "\n",
    "# Create training ImageDataGenerator object\n",
    "if apply_data_augmentation:\n",
    "    train_data_gen = ImageDataGenerator(rotation_range=10,\n",
    "                                        width_shift_range=10,\n",
    "                                        height_shift_range=10,\n",
    "                                        zoom_range=0.3,\n",
    "                                        horizontal_flip=True,\n",
    "                                        vertical_flip=True,\n",
    "                                        fill_mode='constant',\n",
    "                                        cval=0,\n",
    "                                        rescale=1./255)\n",
    "else:\n",
    "    train_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create validation and test ImageDataGenerator objects\n",
    "valid_data_gen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5052 images belonging to 3 classes.\n",
      "Found 562 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create generators to read images from dataset directory\n",
    "# -------------------------------------------------------\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "dataset_dir = os.path.join(cwd, 'data')\n",
    "\n",
    "# img\n",
    "\n",
    "# Batch size\n",
    "bs = 32\n",
    "\n",
    "# img shape\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "\n",
    "num_classes=3\n",
    "\n",
    "# Training\n",
    "training_dir = os.path.join(dataset_dir, 'training')\n",
    "train_gen = train_data_gen.flow_from_directory(training_dir,\n",
    "                                               batch_size=bs, \n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=True,\n",
    "                                               target_size=(img_h, img_w),\n",
    "                                               seed=SEED)  # targets are directly converted into one-hot vectors\n",
    "\n",
    "# Validation\n",
    "validation_dir = os.path.join(dataset_dir, 'validation')\n",
    "valid_gen = valid_data_gen.flow_from_directory(validation_dir,\n",
    "                                               batch_size=bs, \n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=False,\n",
    "                                               target_size=(img_h, img_w),\n",
    "                                               seed=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset objects\n",
    "# ----------------------\n",
    "size = (256, 256)\n",
    "# Training\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
    "\n",
    "# Shuffle (Already done in generator..)\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=len(train_gen))\n",
    "\n",
    "# Normalize images (Already done in generator..)\n",
    "# def normalize_img(x_, y_):\n",
    "#     return tf.cast(x_, tf.float32) / 255., y_\n",
    "\n",
    "# train_dataset = train_dataset.map(normalize_img)\n",
    "\n",
    "# 1-hot encoding <- for categorical cross entropy (Already done in generator..)\n",
    "# def to_categorical(x_, y_):\n",
    "#     return x_, tf.one_hot(y_, depth=10)\n",
    "\n",
    "# train_dataset = train_dataset.map(to_categorical)\n",
    "\n",
    "# Divide in batches (Already done in generator..)\n",
    "# train_dataset = train_dataset.batch(bs)\n",
    "\n",
    "# Repeat\n",
    "# Without calling the repeat function the dataset \n",
    "# will be empty after consuming all the images\n",
    "#Resize performed here both for training and validation\n",
    "train_dataset = train_dataset.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "# Validation\n",
    "# ----------\n",
    "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
    "\n",
    "# Repeat\n",
    "valid_dataset = valid_dataset.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "valid_dataset = valid_dataset.repeat()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 16)      448       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 256, 256, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      4640      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 1,574,307\n",
      "Trainable params: 1,574,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Architecture: Features extraction -> Classifier\n",
    "#Features extraction: we have 6 blocks with an initial number of filters that is 16, it's doubled in each block.\n",
    "#Each block has a conv2D, an activation and a MaxPool2D, we tried to improve the complexity of this part adding more blocks\n",
    "#and filters in order to obtain better features to feed the last part.\n",
    "#Classifier: we subistituted the fully connected part with a GAP layer linked to the dense with the softmax and\n",
    "#we still got better results.\n",
    "\n",
    "start_f = 16\n",
    "depth = 6\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Features extraction\n",
    "for i in range(depth):\n",
    "\n",
    "    if i == 0:\n",
    "        input_shape = [img_h, img_w, 3]\n",
    "    else:\n",
    "        input_shape=[None]\n",
    "\n",
    "    # Conv block: Conv2D -> Activation -> Pooling\n",
    "    model.add(tf.keras.layers.Conv2D(filters=start_f, \n",
    "                                     kernel_size=(3, 3),\n",
    "                                     strides=(1, 1),\n",
    "                                     padding='same',\n",
    "                                     input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "    start_f *= 2\n",
    "    \n",
    "# Classifier\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 158 steps, validate for 18 steps\n",
      "Epoch 1/80\n",
      "158/158 [==============================] - 136s 861ms/step - loss: 1.0995 - accuracy: 0.3325 - val_loss: 1.0982 - val_accuracy: 0.3470\n",
      "Epoch 2/80\n",
      "158/158 [==============================] - 100s 633ms/step - loss: 1.0981 - accuracy: 0.3502 - val_loss: 1.0995 - val_accuracy: 0.3416\n",
      "Epoch 3/80\n",
      "158/158 [==============================] - 109s 688ms/step - loss: 1.0982 - accuracy: 0.3304 - val_loss: 1.0971 - val_accuracy: 0.3754\n",
      "Epoch 4/80\n",
      "158/158 [==============================] - 96s 606ms/step - loss: 1.0977 - accuracy: 0.3484 - val_loss: 1.0982 - val_accuracy: 0.3452\n",
      "Epoch 5/80\n",
      "158/158 [==============================] - 113s 717ms/step - loss: 1.0979 - accuracy: 0.3494 - val_loss: 1.0956 - val_accuracy: 0.3772\n",
      "Epoch 6/80\n",
      "158/158 [==============================] - 97s 613ms/step - loss: 1.0959 - accuracy: 0.3709 - val_loss: 1.1010 - val_accuracy: 0.3470\n",
      "Epoch 7/80\n",
      "158/158 [==============================] - 110s 698ms/step - loss: 1.0937 - accuracy: 0.3717 - val_loss: 1.0982 - val_accuracy: 0.3754\n",
      "Epoch 8/80\n",
      "158/158 [==============================] - 112s 710ms/step - loss: 1.0926 - accuracy: 0.3743 - val_loss: 1.0935 - val_accuracy: 0.3808\n",
      "Epoch 9/80\n",
      "158/158 [==============================] - 135s 851ms/step - loss: 1.0864 - accuracy: 0.3931 - val_loss: 1.0893 - val_accuracy: 0.3630\n",
      "Epoch 10/80\n",
      "158/158 [==============================] - 106s 672ms/step - loss: 1.0749 - accuracy: 0.4097 - val_loss: 1.0718 - val_accuracy: 0.4217\n",
      "Epoch 11/80\n",
      "158/158 [==============================] - 136s 860ms/step - loss: 1.0577 - accuracy: 0.4402 - val_loss: 1.0654 - val_accuracy: 0.3968\n",
      "Epoch 12/80\n",
      "158/158 [==============================] - 122s 772ms/step - loss: 1.0232 - accuracy: 0.4689 - val_loss: 1.0052 - val_accuracy: 0.4858\n",
      "Epoch 13/80\n",
      "158/158 [==============================] - 111s 704ms/step - loss: 0.9548 - accuracy: 0.5241 - val_loss: 0.9613 - val_accuracy: 0.5356\n",
      "Epoch 14/80\n",
      "158/158 [==============================] - 132s 834ms/step - loss: 0.9095 - accuracy: 0.5475 - val_loss: 0.9152 - val_accuracy: 0.5356\n",
      "Epoch 15/80\n",
      "158/158 [==============================] - 113s 716ms/step - loss: 0.8792 - accuracy: 0.5663 - val_loss: 0.9125 - val_accuracy: 0.5374\n",
      "Epoch 16/80\n",
      "158/158 [==============================] - 123s 775ms/step - loss: 0.8387 - accuracy: 0.5891 - val_loss: 0.9486 - val_accuracy: 0.5409\n",
      "Epoch 17/80\n",
      "158/158 [==============================] - 129s 814ms/step - loss: 0.8279 - accuracy: 0.5950 - val_loss: 0.8509 - val_accuracy: 0.5694\n",
      "Epoch 18/80\n",
      "158/158 [==============================] - 113s 715ms/step - loss: 0.8047 - accuracy: 0.6101 - val_loss: 0.8964 - val_accuracy: 0.5747\n",
      "Epoch 19/80\n",
      "158/158 [==============================] - 131s 832ms/step - loss: 0.7805 - accuracy: 0.6251 - val_loss: 0.8767 - val_accuracy: 0.5943\n",
      "Epoch 20/80\n",
      "158/158 [==============================] - 105s 664ms/step - loss: 0.7667 - accuracy: 0.6295 - val_loss: 0.8010 - val_accuracy: 0.6085\n",
      "Epoch 21/80\n",
      "158/158 [==============================] - 134s 849ms/step - loss: 0.7420 - accuracy: 0.6419 - val_loss: 0.8036 - val_accuracy: 0.6263\n",
      "Epoch 22/80\n",
      "158/158 [==============================] - 123s 777ms/step - loss: 0.7424 - accuracy: 0.6502 - val_loss: 0.7645 - val_accuracy: 0.6263\n",
      "Epoch 23/80\n",
      "158/158 [==============================] - 127s 805ms/step - loss: 0.7099 - accuracy: 0.6629 - val_loss: 0.7883 - val_accuracy: 0.6068\n",
      "Epoch 24/80\n",
      "158/158 [==============================] - 130s 826ms/step - loss: 0.7075 - accuracy: 0.6625 - val_loss: 0.7148 - val_accuracy: 0.6459\n",
      "Epoch 25/80\n",
      "158/158 [==============================] - 109s 692ms/step - loss: 0.6923 - accuracy: 0.6740 - val_loss: 0.7361 - val_accuracy: 0.6530\n",
      "Epoch 26/80\n",
      "158/158 [==============================] - 138s 875ms/step - loss: 0.6811 - accuracy: 0.6880 - val_loss: 0.8662 - val_accuracy: 0.5801\n",
      "Epoch 27/80\n",
      "158/158 [==============================] - 119s 756ms/step - loss: 0.6590 - accuracy: 0.6882 - val_loss: 0.6813 - val_accuracy: 0.6690\n",
      "Epoch 28/80\n",
      "158/158 [==============================] - 111s 702ms/step - loss: 0.6549 - accuracy: 0.6928 - val_loss: 0.6975 - val_accuracy: 0.6744\n",
      "Epoch 29/80\n",
      "158/158 [==============================] - 103s 652ms/step - loss: 0.6505 - accuracy: 0.6985 - val_loss: 0.7857 - val_accuracy: 0.6246\n",
      "Epoch 30/80\n",
      "158/158 [==============================] - 99s 628ms/step - loss: 0.6245 - accuracy: 0.7039 - val_loss: 0.6395 - val_accuracy: 0.6957\n",
      "Epoch 31/80\n",
      "158/158 [==============================] - 107s 678ms/step - loss: 0.6233 - accuracy: 0.7114 - val_loss: 0.7593 - val_accuracy: 0.6299\n",
      "Epoch 32/80\n",
      "158/158 [==============================] - 93s 590ms/step - loss: 0.6152 - accuracy: 0.7171 - val_loss: 0.6694 - val_accuracy: 0.6922\n",
      "Epoch 33/80\n",
      "158/158 [==============================] - 109s 692ms/step - loss: 0.6003 - accuracy: 0.7193 - val_loss: 0.6258 - val_accuracy: 0.6922\n",
      "Epoch 34/80\n",
      "158/158 [==============================] - 91s 576ms/step - loss: 0.5897 - accuracy: 0.7304 - val_loss: 0.6320 - val_accuracy: 0.6797\n",
      "Epoch 35/80\n",
      "158/158 [==============================] - 107s 677ms/step - loss: 0.5894 - accuracy: 0.7251 - val_loss: 0.6937 - val_accuracy: 0.6601\n",
      "Epoch 36/80\n",
      "158/158 [==============================] - 104s 659ms/step - loss: 0.5816 - accuracy: 0.7330 - val_loss: 0.6482 - val_accuracy: 0.6797\n",
      "Epoch 37/80\n",
      "158/158 [==============================] - 97s 613ms/step - loss: 0.5696 - accuracy: 0.7365 - val_loss: 0.6164 - val_accuracy: 0.7028\n",
      "Epoch 38/80\n",
      "158/158 [==============================] - 106s 670ms/step - loss: 0.5695 - accuracy: 0.7354 - val_loss: 0.6380 - val_accuracy: 0.6833\n",
      "Epoch 39/80\n",
      "158/158 [==============================] - 94s 596ms/step - loss: 0.5578 - accuracy: 0.7383 - val_loss: 0.6257 - val_accuracy: 0.6833\n",
      "Epoch 40/80\n",
      "158/158 [==============================] - 107s 676ms/step - loss: 0.5568 - accuracy: 0.7460 - val_loss: 0.5867 - val_accuracy: 0.7100\n",
      "Epoch 41/80\n",
      "158/158 [==============================] - 107s 677ms/step - loss: 0.5547 - accuracy: 0.7429 - val_loss: 0.7054 - val_accuracy: 0.6690\n",
      "Epoch 42/80\n",
      "158/158 [==============================] - 114s 722ms/step - loss: 0.5428 - accuracy: 0.7581 - val_loss: 0.6539 - val_accuracy: 0.6762\n",
      "Epoch 43/80\n",
      "158/158 [==============================] - 96s 604ms/step - loss: 0.5326 - accuracy: 0.7595 - val_loss: 0.5988 - val_accuracy: 0.7011\n",
      "Epoch 44/80\n",
      "158/158 [==============================] - 135s 854ms/step - loss: 0.5226 - accuracy: 0.7720 - val_loss: 0.6808 - val_accuracy: 0.6815\n",
      "Epoch 45/80\n",
      "158/158 [==============================] - 98s 622ms/step - loss: 0.5282 - accuracy: 0.7593 - val_loss: 0.6076 - val_accuracy: 0.7117\n",
      "Epoch 46/80\n",
      "158/158 [==============================] - 116s 736ms/step - loss: 0.5215 - accuracy: 0.7633 - val_loss: 0.7319 - val_accuracy: 0.6548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec07c665c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-4\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "\n",
    "metrics = ['accuracy']\n",
    "# ------------------\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "cwd = os.path.join(os.getcwd(),\"drive/My Drive/logs\")\n",
    "\n",
    "exps_dir = os.path.join(cwd, 'custom_1')\n",
    "if not os.path.exists(exps_dir):\n",
    "    os.makedirs(exps_dir)\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "model_name = 'custom'\n",
    "\n",
    "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "    \n",
    "callbacks = []\n",
    "\n",
    "#ReduceLROnPlateau\n",
    "LROnPlateau = False\n",
    "if LROnPlateau:\n",
    "    LROP_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", verbose=1,mode=\"auto\")\n",
    "    callbacks.append(LROP_callback)\n",
    "    \n",
    "\n",
    "\n",
    "# Early Stopping\n",
    "# --------------\n",
    "early_stop = True\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True )\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "epochs = 80\n",
    "model.fit(x=train_dataset, epochs=epochs,steps_per_epoch=len(train_gen),\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=len(valid_gen), \n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 450 validated image filenames belonging to 1 classes.\n",
      "WARNING:tensorflow:From <ipython-input-8-66dfbdbf2ee2>:35: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "15/15 [==============================] - 9s 583ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(key + ',' + str(value) + '\\n')\n",
    "\n",
    "\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "images = [f for f in os.listdir(test_dir)]\n",
    "images = pd.DataFrame(images)\n",
    "images.rename(columns = {0:'filename'}, inplace = True)\n",
    "images[\"class\"] = 'test'\n",
    "\n",
    "test_gen = train_data_gen.flow_from_dataframe(images,\n",
    "                                               test_dir,\n",
    "                                               batch_size=bs,\n",
    "                                               target_size=(img_h, img_w),\n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=False,\n",
    "                                               seed=SEED)\n",
    "\n",
    "\n",
    "test_gen.reset()\n",
    "\n",
    "predictions = model.predict_generator(test_gen, len(test_gen), verbose=1)\n",
    "\n",
    "results = {}\n",
    "images = test_gen.filenames\n",
    "i = 0\n",
    "\n",
    "for p in predictions:\n",
    "  prediction = np.argmax(p)\n",
    "  import ntpath\n",
    "  image_name = ntpath.basename(images[i])\n",
    "  results[image_name] = str(prediction)\n",
    "  i = i + 1\n",
    "\n",
    "create_csv(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
