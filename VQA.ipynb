{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VQAfirstversion.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hK3wsDDllEik","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612022818980,"user_tz":-60,"elapsed":501,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}},"outputId":"0ef359b2-847d-4fab-c3bc-08d062c4979c"},"source":["from IPython.core.interactiveshell import InteractiveShell\r\n","InteractiveShell.ast_node_interactivity = \"all\"\r\n","%tensorflow_version 2.1.2"],"execution_count":1,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `2.1.2`. This will be interpreted as: `2.x`.\n","\n","\n","TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SmrvY8XomcSa","executionInfo":{"status":"ok","timestamp":1612022829960,"user_tz":-60,"elapsed":6139,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["import os\r\n","import tensorflow as tf\r\n","from tensorflow.keras.callbacks import TensorBoard\r\n","from tensorflow.keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU, add, Conv2D, Reshape\r\n","from tensorflow.keras.callbacks import Callback\r\n","from tensorflow.keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\r\n","from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D, multiply\r\n","from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks\r\n","from tensorflow.keras.callbacks import LearningRateScheduler\r\n","from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\r\n","from tensorflow.keras.models import Model\r\n","from tensorflow.keras.metrics import CategoricalAccuracy\r\n","from tensorflow.keras.optimizers import Adam\r\n","from tensorflow.keras.applications import VGG16, VGG19\r\n","from tensorflow.keras.applications.vgg19 import preprocess_input\r\n","from tensorflow.keras.utils import Sequence\r\n","from tensorflow.keras import utils\r\n","from tensorflow.keras.utils import plot_model\r\n","from tensorflow.keras.preprocessing import image, text, sequence\r\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n","from tensorflow.keras.layers import Layer\r\n","from tensorflow.keras import backend as K\r\n","K.set_image_data_format('channels_first')\r\n","\r\n","\r\n","import numpy as np\r\n","\r\n","SEED = 1234\r\n","np.random.seed(SEED)\r\n","tf.random.set_seed(SEED)  \r\n","\r\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"llAVX2V9mcd0","executionInfo":{"status":"ok","timestamp":1612022831278,"user_tz":-60,"elapsed":1715,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["from os import listdir, makedirs\r\n","from os.path import isfile, join, basename, splitext\r\n","from random import seed, shuffle\r\n","import glob\r\n","# set the matplotlib backend so figures can be saved in the background\r\n","import matplotlib\r\n","import matplotlib.pyplot as plt\r\n","matplotlib.use(\"Agg\")\r\n","# import the necessary packages\r\n","\r\n","from sklearn.model_selection import StratifiedShuffleSplit\r\n","from sklearn.utils.class_weight import compute_class_weight\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn.metrics import accuracy_score, f1_score\r\n","from sklearn import preprocessing\r\n","from sklearn.metrics import classification_report\r\n","\r\n","from imutils import paths\r\n","import math\r\n","import numpy as np\r\n","import pickle\r\n","import operator\r\n","from operator import itemgetter\r\n","from itertools import zip_longest\r\n","from collections import defaultdict\r\n","import json\r\n","import joblib\r\n","from tqdm import tqdm\r\n","import pandas as pd\r\n","from nltk.tokenize.treebank import TreebankWordTokenizer\r\n","import pandas as pd\r\n","import seaborn as sns\r\n","import datetime"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NI7y6s_rm229","executionInfo":{"status":"ok","timestamp":1612009819607,"user_tz":-60,"elapsed":421,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}},"outputId":"fc892ec0-7928-4986-a465-87a1f4b4cb2b"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":139,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lFYobkfsm8of","executionInfo":{"status":"ok","timestamp":1611996736747,"user_tz":-60,"elapsed":212872,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["%%capture\r\n","!unzip /content/drive/My\\ Drive/anndl-2020-vqa.zip"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"xSpVK_H-M8MN","executionInfo":{"status":"ok","timestamp":1612022859927,"user_tz":-60,"elapsed":678,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["from os import listdir, makedirs\r\n","from os.path import isfile, join, basename, splitext\r\n","from random import seed, shuffle\r\n","from PIL import Image\r\n","import json\r\n","import cv2\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","\r\n","imgs_path = '/content/VQA_Dataset/Images'\r\n","train_json_path = '/content/VQA_Dataset/train_questions_annotations.json'\r\n","test_json_path = '/content/VQA_Dataset/test_questions.json'\r\n","  \r\n","DATASET_SPLIT = 0.8\r\n","BATCH_SIZE = 128\r\n","  \r\n","classes = {'0': 0,\r\n","            '1': 1,\r\n","            '2': 2,\r\n","            '3': 3,\r\n","            '4': 4,\r\n","            '5': 5,\r\n","            'apple': 6,\r\n","            'baseball': 7,\r\n","            'bench': 8,\r\n","            'bike': 9,\r\n","            'bird': 10,\r\n","            'black': 11,\r\n","            'blanket': 12,\r\n","            'blue': 13,\r\n","            'bone': 14,\r\n","            'book': 15,\r\n","            'boy': 16,\r\n","            'brown': 17,\r\n","            'cat': 18,\r\n","            'chair': 19,\r\n","            'couch': 20,\r\n","            'dog': 21,\r\n","            'floor': 22,\r\n","            'food': 23,\r\n","            'football': 24,\r\n","            'girl': 25,\r\n","            'grass': 26,\r\n","            'gray': 27,\r\n","            'green': 28,\r\n","            'left': 29,\r\n","            'log': 30,\r\n","            'man': 31,\r\n","            'monkey bars': 32,\r\n","            'no': 33,\r\n","            'nothing': 34,\r\n","            'orange': 35,\r\n","            'pie': 36,\r\n","            'plant': 37,\r\n","            'playing': 38,\r\n","            'red': 39,\r\n","            'right': 40,\r\n","            'rug': 41,\r\n","            'sandbox': 42,\r\n","            'sitting': 43,\r\n","            'sleeping': 44,\r\n","            'soccer': 45,\r\n","            'squirrel': 46,\r\n","            'standing': 47,\r\n","            'stool': 48,\r\n","            'sunny': 49,\r\n","            'table': 50,\r\n","            'tree': 51,\r\n","            'watermelon': 52,\r\n","            'white': 53,\r\n","            'wine': 54,\r\n","            'woman': 55,\r\n","            'yellow': 56,\r\n","            'yes': 57}\r\n","  \r\n","N_CLASSES = len(classes)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4VBgN6vQxT_","executionInfo":{"status":"ok","timestamp":1612022891402,"user_tz":-60,"elapsed":630,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["def image_feature_extractor(target_path, image_list, BATCH_SIZE):\r\n","\t\"\"\"\r\n","\tExtracts (512, 7, 7)-dimensional CNN features and save them locally\r\n","\r\n","\tInput:\r\n","\t\ttarget_path: path to save the features\r\n","\t\timage_list: image filenames\r\n","\t\tBATCH_SIZE: batch size\r\n","\r\n","\tReturns:\r\n","\t\tNone\r\n","\t\"\"\"\r\n","\t \r\n","\tmodel = VGG19(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(3, 224, 224)))\r\n","\r\n","  \t# add a progress bar\r\n","\tprogbar = utils.Progbar(int(np.ceil(len(image_list) / float(BATCH_SIZE))))\r\n","\r\n","  \t# loop over the images in batches\r\n","\tfor (b, i) in enumerate(range(0, len(image_list), BATCH_SIZE)):\r\n","\t\t# extract batch of images and prepare them to pass it through the VGG \r\n","\t\t# network for feature extraction\r\n","\r\n","\t\tprogbar.update(b+1)\r\n","\t\t\r\n","\t\tbatch_range = range(i, min(i + BATCH_SIZE, len(image_list)))\r\n","\t\tbatchPaths = image_list[batch_range[0]: batch_range[-1]+1]\r\n","\r\n","\t\tbatchImages = []\r\n","\t\tbatchIds = []\r\n","\t\t# loop over the images and labels in the current batch\r\n","\t\tfor imagePath in batchPaths:\r\n","\r\n","            # load the input image using the Keras helper utility\r\n","            # while ensuring the image is resized to 224x224 pixels\r\n","\t\t\timg = image.load_img(os.path.join(imgs_path,imagePath), target_size=(224, 224))\r\n","\t\t\timg = image.img_to_array(img)\r\n","    \r\n","            # preprocess the image by \r\n","            # (1) expanding the dimensions to include batch dim and\r\n","            # (2) subtracting the mean RGB pixel intensity from the ImageNet dataset\r\n","\t\t\timg = np.expand_dims(img, axis=0)\r\n","\t\t\timg = preprocess_input(img)\r\n","    \r\n","            # add the image to the batch\r\n","\t\t\tbatchImages.append(img)\r\n","\t\t\t# image ids of the batch\r\n","\t\t\tbatchIds.append(imagePath.split('.')[0][-6:])\r\n","\t  \r\n","\t\tbatchImages = np.vstack(batchImages) # (BATCH_SIZE, 3, 224, 224)\r\n","\r\n","\t\t# pass the images through the network and use the outputs as our actual features\r\n","\t\tfeatures = model.predict(batchImages) # (BATCH_SIZE, 512, 7, 7)\r\n","\t\tfeatures = tf.reshape(features, (features.shape[0], features.shape[1], -1)) # (BATCH_SIZE, 512, 49)\r\n","\t\tfeatures = tf.transpose(features, perm =[0,2,1])  # (BATCH_SIZE, 49, 512)\r\n","\r\n","\t\t# loop over the batch to save them locally\r\n","\t\tfor id, feat in zip(batchIds, features):\r\n","\t\t\tnp.save(os.path.join(target_path, id), feat)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"qbl912KkSLsz","executionInfo":{"status":"ok","timestamp":1612022902270,"user_tz":-60,"elapsed":535,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["image_list = os.listdir(\"/content/VQA_Dataset/Images\")\r\n","BATCH_SIZE = 300\r\n","target_path = '/content/drive/My Drive/VQA/features'"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zl3ThJ6kSU0M","executionInfo":{"status":"ok","timestamp":1612014710109,"user_tz":-60,"elapsed":804194,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}},"outputId":"9c6ac8f9-ab58-4b40-c414-d4fb87c766fe"},"source":["image_feature_extractor(target_path, image_list, BATCH_SIZE)"],"execution_count":169,"outputs":[{"output_type":"stream","text":["98/98 [==============================] - 794s 8s/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"02QIzaKv4a6_","executionInfo":{"status":"ok","timestamp":1612022904220,"user_tz":-60,"elapsed":788,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["from tensorflow.keras.applications.vgg19 import preprocess_input\r\n","from tensorflow.keras.preprocessing.text import Tokenizer\r\n","\r\n","\r\n","def process_sentence(sentence):\r\n","    \"\"\"\r\n","    Cleans a given raw sentence\r\n","    Input:\r\n","        sentence: a raw sentence\r\n","    Returns:\r\n","        Returns the cleaned version of the sentence\r\n","    \"\"\"\r\n","    # remove the character \".\", except from floating numbers\r\n","    periodStrip  = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\r\n","    # remove any \",\" between digits, eg: 5,6\r\n","    commaStrip   = re.compile(\"(\\d)(\\,)(\\d)\")\r\n","    # list of punctuations to remove\r\n","    punct        = [';', r\"/\", '[', ']', '\"', '{', '}',\r\n","                    '(', ')', '=', '+', '\\\\', '_', '-',\r\n","                    '*', ':', '^', '%', '$', '#', '&',\r\n","                    '>', '<', '@', '`', ',', '?', '!']\r\n","    # contraction mappings\r\n","    contractions = {\"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\": \"could've\", \"couldnt\": \"couldn't\", \\\r\n","                    \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\": \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\", \\\r\n","                    \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\": \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\", \\\r\n","                    \"he'dve\": \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\", \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", \\\r\n","                    \"Im\": \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\": \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\", \\\r\n","                    \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\": \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\", \\\r\n","                    \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\", \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\", \\\r\n","                    \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\", \\\r\n","                    \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\": \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\", \\\r\n","                    \"somebody'd\": \"somebodyd\", \"somebodyd've\": \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\": \"somebody'll\", \\\r\n","                    \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\", \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\", \\\r\n","                    \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\": \"something'd\", \"somethingd've\": \"something'd've\", \\\r\n","                    \"something'dve\": \"something'd've\", \"somethingll\": \"something'll\", \"thats\": \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\", \\\r\n","                    \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\": \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\", \\\r\n","                    \"they'dve\": \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\": \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\", \\\r\n","                    \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\": \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\", \\\r\n","                    \"whats\": \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\": \"where'd\", \"where's\": \"where is\", \"whereve\": \"where've\", \\\r\n","                    \"whod\": \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\": \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\", \\\r\n","                    \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\": \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\", \\\r\n","                    \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\", \\\r\n","                    \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\": \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", \\\r\n","                    \"youll\": \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"}\r\n","\r\n","    # replace new line with a white space\r\n","    inText = sentence.replace('\\n', ' ')\r\n","    # replace multiple white space with single white space\r\n","    inText = inText.replace('\\t', ' ')\r\n","    inText = inText.strip()\r\n","    outText = inText\r\n","    for p in punct:\r\n","        if (p + ' ' in inText or ' ' + p in inText) or \\\r\n","           (re.search(commaStrip, inText) != None):\r\n","            outText = outText.replace(p, '')\r\n","        else:\r\n","            outText = outText.replace(p, ' ')\r\n","    outText = periodStrip.sub(\"\", outText, re.UNICODE)\r\n","    outText = outText.lower().split()\r\n","    for wordId, word in enumerate(outText):\r\n","        if word in contractions:\r\n","            outText[wordId] = contractions[word]\r\n","    outText = ' '.join(outText)\r\n","    return outText\r\n","\r\n","\r\n","\r\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyJgzHYaNeWa","executionInfo":{"status":"ok","timestamp":1612022913307,"user_tz":-60,"elapsed":551,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["#read train JSON file\r\n","with open(train_json_path, 'r') as f:\r\n","    train_data = json.load(f)\r\n","f.close()\r\n","\r\n","#read test JSON file\r\n","with open(test_json_path, 'r') as f:\r\n","    test_data = json.load(f)\r\n","f.close()\r\n","\r\n","\r\n","TOT_QUESTIONS = len(train_data)\r\n","indx = list(train_data.keys())\r\n","train_indx = indx[:int(TOT_QUESTIONS*DATASET_SPLIT)]\r\n","valid_indx = indx[int(TOT_QUESTIONS*DATASET_SPLIT):]\r\n","\r\n","\r\n","\r\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f3KTeFXqoQEq","executionInfo":{"status":"ok","timestamp":1612022918557,"user_tz":-60,"elapsed":4547,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}},"outputId":"9e1ab453-b96c-4f0e-9b6e-cfeabb0c3d26"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\r\n","import pandas as pd\r\n","import re\r\n","\r\n","questions=[]\r\n","for ind in indx:\r\n","  questions.append(train_data[ind][\"question\"])\r\n","test_indx = list(test_data.keys())\r\n","for ind in test_indx:\r\n","  questions.append(test_data[ind][\"question\"])\r\n","\r\n","questions_processed = pd.Series(questions).apply(process_sentence)\r\n","\r\n","MAX_SEQ = 95\r\n","tokenizer = Tokenizer(filters='')\r\n","tokenizer.fit_on_texts(questions_processed)\r\n","vocab_size = len(tokenizer.word_index) + 1\r\n","print(f'Vocab Size: {vocab_size}')\r\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Vocab Size: 4641\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R2FO27yTggKb","executionInfo":{"status":"ok","timestamp":1612013838360,"user_tz":-60,"elapsed":798,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["# save to disk\r\n","import joblib\r\n","with open('/content/drive/My Drive/VQA/text_tokenizer.pkl', 'wb') as f:\r\n","   joblib.dump(tokenizer, f)"],"execution_count":167,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzh2pvEAat7M","executionInfo":{"status":"ok","timestamp":1612022925303,"user_tz":-60,"elapsed":3769,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["train_questions=[]\r\n","\r\n","for indx in train_data.keys():\r\n","  train_questions.append(train_data[indx][\"question\"])\r\n","\r\n","\r\n","questions_train_processed  = pd.Series(train_questions).apply(process_sentence)\r\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tuIsup3HalLc","executionInfo":{"status":"ok","timestamp":1612022948575,"user_tz":-60,"elapsed":1383,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}},"outputId":"0b9592db-61f2-4bc7-e2ea-d6f6173d7ccc"},"source":["import matplotlib\r\n","import seaborn as sns\r\n","import matplotlib.pyplot as plt\r\n","question_data_train = tokenizer.texts_to_sequences(questions_train_processed)\r\n","question_len = [len(text) for text in question_data_train]\r\n","plt.figure(figsize=(7,5))\r\n","sns.distplot(question_len, color='red')\r\n","plt.title('Distribution of Question length')\r\n","plt.xlabel('Length of Question')\r\n","plt.ylabel('Question count')\r\n","plt.xlim(0, 30)\r\n","plt.show()"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Figure size 504x360 with 0 Axes>"]},"metadata":{"tags":[]},"execution_count":11},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n","  warnings.warn(msg, FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f18eacf3a20>"]},"metadata":{"tags":[]},"execution_count":11},{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Distribution of Question length')"]},"metadata":{"tags":[]},"execution_count":11},{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 0, 'Length of Question')"]},"metadata":{"tags":[]},"execution_count":11},{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'Question count')"]},"metadata":{"tags":[]},"execution_count":11},{"output_type":"execute_result","data":{"text/plain":["(0.0, 30.0)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kTjgwU1rcrPW","executionInfo":{"status":"ok","timestamp":1612022957205,"user_tz":-60,"elapsed":527,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}},"outputId":"74d37714-9695-4061-92b7-fa477ad6d032"},"source":["\r\n","for i in range(0,11):\r\n","    print(10*i,'percentile value is', np.percentile(question_len,10*i))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["0 percentile value is 2.0\n","10 percentile value is 4.0\n","20 percentile value is 5.0\n","30 percentile value is 5.0\n","40 percentile value is 5.0\n","50 percentile value is 6.0\n","60 percentile value is 6.0\n","70 percentile value is 7.0\n","80 percentile value is 8.0\n","90 percentile value is 9.0\n","100 percentile value is 21.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FG658rnWcnlO","executionInfo":{"status":"ok","timestamp":1612022958807,"user_tz":-60,"elapsed":559,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}},"outputId":"346e1b85-652f-41be-b717-1b175c988292"},"source":["\r\n","for i in range(0,11):\r\n","    print(90+i,'percentile value is',np.percentile(question_len,90+i))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["90 percentile value is 9.0\n","91 percentile value is 9.0\n","92 percentile value is 9.0\n","93 percentile value is 9.0\n","94 percentile value is 10.0\n","95 percentile value is 10.0\n","96 percentile value is 10.0\n","97 percentile value is 11.0\n","98 percentile value is 11.0\n","99 percentile value is 13.0\n","100 percentile value is 21.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"as9aSsF1cvjt","executionInfo":{"status":"ok","timestamp":1612022961239,"user_tz":-60,"elapsed":486,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["MAX_LEN = 22\r\n","\r\n","question_data_train=sequence.pad_sequences(question_data_train, maxlen=MAX_LEN, padding='post')\r\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8vSyznaYnVUz","executionInfo":{"status":"ok","timestamp":1612022974306,"user_tz":-60,"elapsed":527,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}},"outputId":"827fc881-2447-427e-9db8-bde2ee79a289"},"source":["answer_train = []\r\n","for indx in train_data.keys():\r\n","  answer_train.append(train_data[indx][\"answer\"])\r\n","\r\n","labelencoder = preprocessing.LabelEncoder()\r\n","labelencoder.fit(answer_train)\r\n","\r\n","print(len(labelencoder.classes_))"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LabelEncoder()"]},"metadata":{"tags":[]},"execution_count":15},{"output_type":"stream","text":["58\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eMwIVNshnvhw","executionInfo":{"status":"ok","timestamp":1612015690417,"user_tz":-60,"elapsed":697,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["with open('/content/drive/My Drive/VQA/labelencoder.pkl', 'wb') as f:\r\n","  joblib.dump(labelencoder, f)"],"execution_count":184,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z8rx6XhODzKS","executionInfo":{"status":"ok","timestamp":1612023021637,"user_tz":-60,"elapsed":575,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["def get_answers_matrix(answers, encoder):\r\n","\t'''\r\n","\tOne-hot-encodes the answers\r\n","\r\n","\tInput:\r\n","\t\tanswers:\tlist of answer\r\n","\t\tencoder:\ta scikit-learn LabelEncoder object\r\n","  \r\n","\tOutput:\r\n","\t\tA numpy array of shape (# of answers, # of class)\r\n","\t'''\r\n","\ty = encoder.transform(answers) #string to numerical class\r\n","\tnb_classes = encoder.classes_.shape[0]\r\n","\tY = utils.to_categorical(y, nb_classes)\r\n","\treturn Y"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PPSI1fMoItr","executionInfo":{"status":"ok","timestamp":1612022983244,"user_tz":-60,"elapsed":640,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["questions_train = list(map(itemgetter('question'), train_data.values()))\r\n","answer_train = list(map(itemgetter('answer'), train_data.values()))\r\n","images_train =  list(map(itemgetter('image_id'), train_data.values()))"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"E3jRLe85n73Q","executionInfo":{"status":"ok","timestamp":1612022984262,"user_tz":-60,"elapsed":526,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["sss = StratifiedShuffleSplit(n_splits=1, test_size= 0.20,random_state=42)\r\n","\r\n","for train_index, val_index in sss.split(images_train, answer_train):\r\n","  TRAIN_INDEX = train_index\r\n","  VAL_INDEX = val_index"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"y3OGoxiWpl-Q","executionInfo":{"status":"ok","timestamp":1612022985451,"user_tz":-60,"elapsed":722,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["# image data\r\n","image_list_tr, image_list_vl = np.array(images_train)[TRAIN_INDEX.astype(int)], np.array(images_train)[VAL_INDEX.astype(int)]"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"mRMkp-H0qKv-","executionInfo":{"status":"ok","timestamp":1612023024644,"user_tz":-60,"elapsed":530,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["# answer data\r\n","answer_matrix = get_answers_matrix(answer_train, labelencoder)\r\n","answer_tr, answer_vl = answer_matrix[TRAIN_INDEX], answer_matrix[VAL_INDEX]"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"vWDddyyGp1Zh","executionInfo":{"status":"ok","timestamp":1612023026490,"user_tz":-60,"elapsed":788,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["# question data\r\n","question_tr, question_vl = question_data_train[TRAIN_INDEX], question_data_train[VAL_INDEX]"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUL7-c7psFCr","executionInfo":{"status":"ok","timestamp":1612023031882,"user_tz":-60,"elapsed":719,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["BATCH_SIZE = 300\r\n","BUFFER_SIZE = 5000\r\n","def map_func(img_name, ques, ans):\r\n","    img_tensor = np.load('features/' + img_name.decode('utf-8').split('.')[0][-6:] + '.npy')\r\n","    return img_tensor, ques, ans"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"FjvtcaniuycG","executionInfo":{"status":"ok","timestamp":1612023038065,"user_tz":-60,"elapsed":5126,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["\r\n","dataset_tr = tf.data.Dataset.from_tensor_slices((image_list_tr, question_tr, answer_tr))\r\n","\r\n","# Use map to load the numpy files in parallel\r\n","dataset_tr = dataset_tr.map(lambda item1, item2, item3: tf.numpy_function(\r\n","    map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.float32]),\r\n","    num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n","\r\n","# Shuffle and batch\r\n","dataset_tr = dataset_tr.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n","dataset_tr = dataset_tr.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"3QitSQWCu2cU","executionInfo":{"status":"ok","timestamp":1612023038066,"user_tz":-60,"elapsed":4312,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["dataset_vl = tf.data.Dataset.from_tensor_slices((image_list_vl, question_vl, answer_vl))\r\n","\r\n","# Use map to load the numpy files in parallel\r\n","dataset_vl = dataset_vl.map(lambda item1, item2, item3: tf.numpy_function(\r\n","    map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.float32]),\r\n","    num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n","\r\n","# Shuffle and batch\r\n","dataset_vl = dataset_vl.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n","dataset_vl = dataset_vl.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"w23Wo-LNCegN","executionInfo":{"status":"ok","timestamp":1612023038066,"user_tz":-60,"elapsed":3055,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["class AttentionMaps(tf.keras.layers.Layer):\r\n","  \"\"\"\r\n","  Given an image feature map V ∈ R(d×N), and the question representation Q ∈ R(d×T), \r\n","  calculates the affinity matrix C ∈ R(T×N): C = tanh((QT)(Wb)V) ; \r\n","  where Wb ∈ R(d×d) contains the weights. (Refer eqt (3) section 3.3).\r\n","\r\n","  Given this affinity matrix C ∈ R(T×N), predicts image and question attention maps \r\n","  (Refer eqt (4) section 3.3).\r\n","\r\n","  Arguments:\r\n","    dim_k     : hidden attention dimention\r\n","    reg_value : Regularization value\r\n","\r\n","\r\n","  Inputs:\r\n","    image_feat,    V : shape (N,  d) or (49, dim_d)\r\n","    ques_feat,     Q : shape (T,  d) or (23, dim_d)\r\n","\r\n","  Outputs:\r\n","    Image and Question attention maps viz:\r\n","    a) Hv = tanh(WvV + (WqQ)C) and\r\n","    b) Hq = tanh(WqQ + (WvV )CT)\r\n","  \"\"\"\r\n","  def __init__(self, dim_k, reg_value, **kwargs):\r\n","    super(AttentionMaps, self).__init__(**kwargs)\r\n","\r\n","    self.dim_k = dim_k\r\n","    self.reg_value = reg_value\r\n","\r\n","    self.Wv = Dense(self.dim_k, activation=None,\\\r\n","                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\r\n","                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=2))\r\n","    self.Wq = Dense(self.dim_k, activation=None,\\\r\n","                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\r\n","                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=3))\r\n","\r\n","  def call(self, image_feat, ques_feat):\r\n","    \"\"\"\r\n","    The main logic of this layer.\r\n","    \"\"\"  \r\n","\r\n","    # Affinity Matrix C\r\n","    # (QT)(Wb)V \r\n","    C = tf.matmul(ques_feat, tf.transpose(image_feat, perm=[0,2,1])) # [b, 23, 49]\r\n","    # tanh((QT)(Wb)V)\r\n","    C = tf.keras.activations.tanh(C) \r\n","\r\n","    # (Wv)V\r\n","    WvV = self.Wv(image_feat)                             # [b, 49, dim_k]\r\n","    # (Wq)Q\r\n","    WqQ = self.Wq(ques_feat)                              # [b, 23, dim_k]\r\n","\r\n","    # ((Wq)Q)C\r\n","    WqQ_C = tf.matmul(tf.transpose(WqQ, perm=[0,2,1]), C) # [b, k, 49]\r\n","    WqQ_C = tf.transpose(WqQ_C, perm =[0,2,1])            # [b, 49, k]\r\n","\r\n","    # ((Wv)V)CT                                           # [b, k, 23]\r\n","    WvV_C = tf.matmul(tf.transpose(WvV, perm=[0,2,1]), tf.transpose(C, perm=[0,2,1]))  \r\n","                        \r\n","    WvV_C = tf.transpose(WvV_C, perm =[0,2,1])            # [b, 23, k]\r\n","\r\n","    #---------------image attention map------------------\r\n","    # We find \"Hv = tanh((Wv)V + ((Wq)Q)C)\" ; H_v shape [49, k]\r\n","\r\n","    H_v = WvV + WqQ_C                                     # (Wv)V + ((Wq)Q)C\r\n","    H_v = tf.keras.activations.tanh(H_v)                  # tanh((Wv)V + ((Wq)Q)C) \r\n","\r\n","    #---------------question attention map---------------\r\n","    # We find \"Hq = tanh((Wq)Q + ((Wv)V)CT)\" ; H_q shape [23, k]\r\n","\r\n","    H_q = WqQ + WvV_C                                     # (Wq)Q + ((Wv)V)CT\r\n","    H_q = tf.keras.activations.tanh(H_q)                  # tanh((Wq)Q + ((Wv)V)CT) \r\n","        \r\n","    return [H_v, H_q]                                     # [b, 49, k], [b, 23, k]\r\n","  \r\n","  def get_config(self):\r\n","    \"\"\"\r\n","    This method collects the input shape and other information about the layer.\r\n","    \"\"\"\r\n","    config = {\r\n","        'dim_k': self.dim_k,\r\n","        'reg_value': self.reg_value\r\n","    }\r\n","    base_config = super(AttentionMaps, self).get_config()\r\n","    return dict(list(base_config.items()) + list(config.items()))"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"PWQ2hyaOChE8","executionInfo":{"status":"ok","timestamp":1612023038067,"user_tz":-60,"elapsed":2303,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["class ContextVector(tf.keras.layers.Layer):\r\n","  \"\"\"\r\n","  Method to find context vector of the image and text features\r\n","  (Refer eqt (4) and (5) section 3.3).\r\n","  \r\n","  Arguments:\r\n","    reg_value : Regularization value\r\n","    \r\n","  Inputs:\r\n","    image_feat V: image features, (49, d)\r\n","    ques_feat  Q: question features, (23, d)\r\n","    H_v: image attention map, (49, k)\r\n","    H_q: question attention map, (23, k)\r\n","\r\n","  Outputs:\r\n","    Returns d-dimenstional context vector for image and question features\r\n","  \"\"\"\r\n","  def __init__(self, reg_value, **kwargs):\r\n","    super(ContextVector, self).__init__(**kwargs)\r\n","\r\n","    self.reg_value = reg_value\r\n","\r\n","    self.w_hv = Dense(1, activation='softmax',\\\r\n","                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\r\n","                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=4))\r\n","    self.w_hq = Dense(1, activation='softmax',\\\r\n","                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\r\n","                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=5)) \r\n","    \r\n","\r\n","  def call(self, image_feat, ques_feat, H_v, H_q):\r\n","    \"\"\"\r\n","    The main logic of this layer.\r\n","    \"\"\"  \r\n","    # attention probabilities of each image region vn; a_v = softmax(wT_hv * H_v)\r\n","    a_v = self.w_hv(H_v)                               # [b, 49, 1]\r\n","\r\n","    # attention probabilities of each word qt ;        a_q = softmax(wT_hq * H_q)\r\n","    a_q = self.w_hq(H_q)                               # [b, 23, 1]\r\n","\r\n","    # context vector for image\r\n","    v = a_v * image_feat                               # [b, 49, dim_d]\r\n","    v = tf.reduce_sum(v, 1)                            # [b, dim_d]\r\n","\r\n","    # context vector for question\r\n","    q = a_q * ques_feat                                # [b, 23, dim_d]\r\n","    q = tf.reduce_sum(q, 1)                            # [b, dim_d]\r\n","\r\n","\r\n","    return [v, q]\r\n","\r\n","  def get_config(self):\r\n","    \"\"\"\r\n","    This method collects the input shape and other information about the layer.\r\n","    \"\"\"\r\n","    config = {\r\n","        'reg_value': self.reg_value\r\n","    }\r\n","    base_config = super(ContextVector, self).get_config()\r\n","    return dict(list(base_config.items()) + list(config.items()))"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"TcOPFHspCl7A","executionInfo":{"status":"ok","timestamp":1612023038068,"user_tz":-60,"elapsed":1729,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["class PhraseLevelFeatures(tf.keras.layers.Layer):\r\n","  \"\"\"\r\n","  We compute the phrase features by applying 1-D convolution on the word embedding \r\n","  vectors with filters of three window sizes: unigram, bigram and trigram.\r\n","  The word-level features Qw are appropriately 0-padded before feeding into bigram and \r\n","  trigram convolutions to maintain the length of the sequence after convolution.\r\n","  Given the convolution result, we then apply max-pooling across different n-grams at each word\r\n","  location to obtain phrase-level features\r\n","  (Refer eqt (1) and (2) section 3.2).\r\n","\r\n","  Arguments:\r\n","    dim_d: hidden dimension\r\n","\r\n","  Inputs:\r\n","    word_feat Q : word level features of shape (23, dim_d)\r\n","\r\n","  Outputs:\r\n","    Phrase level features of the question of shape (23, dim_d)\r\n","  \"\"\"\r\n","  def __init__(self, dim_d, **kwargs):\r\n","    super(PhraseLevelFeatures, self).__init__(**kwargs)\r\n","    \r\n","    self.dim_d = dim_d\r\n","    \r\n","    self.conv_unigram = Conv1D(self.dim_d, kernel_size=1, strides=1,\\\r\n","                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=6)) \r\n","    self.conv_bigram =  Conv1D(self.dim_d, kernel_size=2, strides=1, padding='same',\\\r\n","                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=7)) \r\n","    self.conv_trigram = Conv1D(self.dim_d, kernel_size=3, strides=1, padding='same',\\\r\n","                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=8)) \r\n","\r\n","\r\n","  def call(self, word_feat):\r\n","    \"\"\"\r\n","    The main logic of this layer.\r\n","\r\n","    Compute the n-gram phrase embeddings (n=1,2,3)\r\n","    \"\"\"\r\n","    # phrase level unigram features\r\n","    x_uni = self.conv_unigram(word_feat)                    # [b, 23, dim_d]\r\n","\r\n","    # phrase level bigram features\r\n","    x_bi  = self.conv_bigram(word_feat)                     # [b, 23, dim_d]\r\n","\r\n","    # phrase level trigram features\r\n","    x_tri = self.conv_trigram(word_feat)                    # [b, 23, dim_d]\r\n","\r\n","    # Concat\r\n","    x = tf.concat([tf.expand_dims(x_uni, -1),\\\r\n","                    tf.expand_dims(x_bi, -1),\\\r\n","                    tf.expand_dims(x_tri, -1)], -1)         # [b, 23, dim_d, 3]\r\n","\r\n","    # https://stackoverflow.com/a/36853403\r\n","    # Max-pool across n-gram features; over-all phrase level feature\r\n","    x = tf.reduce_max(x, -1)                                # [b, 23, dim_d]\r\n","\r\n","    return x\r\n","\r\n","  def get_config(self):\r\n","    \"\"\"\r\n","    This method collects the input shape and other information about the layer.\r\n","    \"\"\"\r\n","    config = {\r\n","        'dim_d': self.dim_d\r\n","    }\r\n","    base_config = super(PhraseLevelFeatures, self).get_config()\r\n","    return dict(list(base_config.items()) + list(config.items()))"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"pXkxw2czCqQj","executionInfo":{"status":"ok","timestamp":1612023040003,"user_tz":-60,"elapsed":713,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["def build_model(max_answers, max_seq_len, vocab_size, dim_d, dim_k, l_rate, d_rate, reg_value):\r\n","    \"\"\"\r\n","    Defines the Keras model.\r\n","\r\n","    Arguments\r\n","    ----------\r\n","    max_answers : Number of output targets of the model.\r\n","    max_seq_len : Length of input sequences\r\n","    vocab_size  : Size of the vocabulary, i.e. maximum integer index + 1.\r\n","    dim_d       : Hidden dimension\r\n","    dim_k       : Hidden attention dimension\r\n","    l_rate      : Learning rate for the model\r\n","    d_rate      : Dropout rate\r\n","    reg_value   : Regularization value\r\n","\r\n","    Returns\r\n","    ----------\r\n","    Returns the Keras model.\r\n","    \"\"\"\r\n","    # inputs \r\n","    image_input = Input(shape=(49, 512, ), name='Image_Input')\r\n","    ques_input = Input(shape=(22, ), name='Question_Input')\r\n","\r\n","    # image feature; (Wb)V                                          # [b, 49, dim_d]\r\n","    image_feat = Dense(dim_d, activation=None, name='Image_Feat_Dense',\\\r\n","                            kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\r\n","                                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=1))(image_input)\r\n","    image_feat = Dropout(d_rate, seed=1)(image_feat)\r\n","\r\n","    # word level\r\n","    ques_feat_w = Embedding(input_dim=vocab_size, output_dim=dim_d, input_length=max_seq_len,\\\r\n","                            mask_zero=True)(ques_input)\r\n","    \r\n","    Hv_w, Hq_w = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Word')(image_feat, ques_feat_w)\r\n","    v_w, q_w = ContextVector(reg_value, name='ContextVector_Word')(image_feat, ques_feat_w, Hv_w, Hq_w)\r\n","    feat_w = tf.add(v_w,q_w)\r\n","    h_w = Dense(dim_d, activation='tanh', name='h_w_Dense',\\\r\n","                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\r\n","                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=13))(feat_w)\r\n","\r\n","    # phrase level\r\n","    ques_feat_p = PhraseLevelFeatures(dim_d, name='PhraseLevelFeatures')(ques_feat_w)\r\n","\r\n","    Hv_p, Hq_p = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Phrase')(image_feat, ques_feat_p)\r\n","    v_p, q_p = ContextVector(reg_value, name='ContextVector_Phrase')(image_feat, ques_feat_p, Hv_p, Hq_p)\r\n","    feat_p = concatenate([tf.add(v_p,q_p), h_w], -1) \r\n","    h_p = Dense(dim_d, activation='tanh', name='h_p_Dense',\\\r\n","                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\r\n","                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=14))(feat_p)\r\n","\r\n","    # sentence level\r\n","    ques_feat_s = LSTM(dim_d, return_sequences=True, input_shape=(None, max_seq_len, dim_d),\\\r\n","                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=16))(ques_feat_p)\r\n","\r\n","    Hv_s, Hq_s = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Sent')(image_feat, ques_feat_s)\r\n","    v_s, q_s = ContextVector(reg_value, name='ContextVector_Sent')(image_feat, ques_feat_p, Hv_s, Hq_s)\r\n","    feat_s = concatenate([tf.add(v_s,q_s), h_p], -1) \r\n","    h_s = Dense(2*dim_d, activation='tanh', name='h_s_Dense',\\\r\n","                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\r\n","                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=15))(feat_s)\r\n","\r\n","    z   = Dense(2*dim_d, activation='tanh', name='z_Dense',\\\r\n","                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\r\n","                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=16))(h_s)\r\n","    z   = Dropout(d_rate, seed=16)(z)\r\n","\r\n","    # result\r\n","    result = Dense(max_answers, activation='softmax')(z)\r\n","\r\n","    model = Model(inputs=[image_input, ques_input], outputs=result)\r\n","\r\n","    return model"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"hH-gy1c1vApW","executionInfo":{"status":"ok","timestamp":1612023086486,"user_tz":-60,"elapsed":540,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}}},"source":["# params 1\r\n","\r\n","EPOCHS      = 60\r\n","max_seq_len = 22\r\n","vocab_size  = len(tokenizer.word_index) + 1\r\n","dim_d       = 512\r\n","dim_k       = 256\r\n","l_rate      = 1e-4\r\n","d_rate      = 0.5\r\n","reg_value   = 0.01\r\n","\r\n","base_path = '/content/drive/My Drive/VQA/'"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"VYPQ0ZcZvI5a","executionInfo":{"status":"error","timestamp":1612023088602,"user_tz":-60,"elapsed":1396,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}},"outputId":"e4f7d298-ecee-42f6-e3e2-37740fa25c27"},"source":["# create model\r\n","model = build_model(N_CLASSES, max_seq_len, vocab_size, dim_d, dim_k, l_rate, d_rate, reg_value)"],"execution_count":32,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1852\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be at least rank 3 but is rank 2 for '{{node BiasAdd}} = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\"](add, bias)' with input shapes: [?,2048], [2048].","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-c9e80e778668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-30-da189b812a86>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(max_answers, max_seq_len, vocab_size, dim_d, dim_k, l_rate, d_rate, reg_value)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# sentence level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mques_feat_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglorot_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mques_feat_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mHv_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHq_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionMaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'AttentionMaps_Sent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mques_feat_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 952\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1091\u001b[0;31m             inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m           (last_output, outputs, new_h, new_c,\n\u001b[0;32m-> 1270\u001b[0;31m            runtime) = lstm_with_backend_selection(**normal_lstm_kwargs)\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m       \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mlstm_with_backend_selection\u001b[0;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   1653\u001b[0m     \u001b[0;31m# Call the normal LSTM impl and register the CuDNN impl function. The\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[0;31m# grappler will kick in during session execution to optimize the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1655\u001b[0;31m     \u001b[0mlast_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefun_standard_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m     \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_gpu_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m-> 2941\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstandard_lstm\u001b[0;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   1400\u001b[0m       input_length=(sequence_lengths\n\u001b[1;32m   1401\u001b[0m                     if sequence_lengths is not None else timesteps),\n\u001b[0;32m-> 1402\u001b[0;31m       zero_output_for_mask=zero_output_for_mask)\n\u001b[0m\u001b[1;32m   1403\u001b[0m   return (last_output, outputs, new_states[0], new_states[1],\n\u001b[1;32m   1404\u001b[0m           _runtime(_RUNTIME_CPU))\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   4347\u001b[0m     \u001b[0;31m# the value is discarded.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4348\u001b[0m     output_time_zero, _ = step_function(\n\u001b[0;32m-> 4349\u001b[0;31m         input_time_zero, tuple(initial_states) + tuple(constants))\n\u001b[0m\u001b[1;32m   4350\u001b[0m     output_ta = tuple(\n\u001b[1;32m   4351\u001b[0m         tensor_array_ops.TensorArray(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(cell_inputs, cell_states)\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_tm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_kernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0mz0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(x, bias, data_format)\u001b[0m\n\u001b[1;32m   5952\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5953\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_first'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5954\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NCHW'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5955\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NHWC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5956\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   3377\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3378\u001b[0m       return gen_nn_ops.bias_add(\n\u001b[0;32m-> 3379\u001b[0;31m           value, bias, data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m   3380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    689\u001b[0m   \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m--> 691\u001b[0;31m         \"BiasAdd\", value=value, bias=bias, data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    692\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    748\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[1;32m    749\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    590\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    591\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3534\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3535\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3536\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3537\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3538\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2014\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0;32m-> 2016\u001b[0;31m                                 control_input_ops, op_def)\n\u001b[0m\u001b[1;32m   2017\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1854\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1856\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Shape must be at least rank 3 but is rank 2 for '{{node BiasAdd}} = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\"](add, bias)' with input shapes: [?,2048], [2048]."]}]},{"cell_type":"code","metadata":{"id":"5-6JZlg7vaN7"},"source":["\r\n","steps_per_epoch = int(np.ceil(len(image_list_tr)/BATCH_SIZE))\r\n","boundaries      = [50*steps_per_epoch]\r\n","values          = [l_rate, l_rate/10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHUneVNPvbUn"},"source":["# we reduce the l_rate after 50th epoch (from 1e-4 to 1e-5)\r\n","learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\r\n","optimizer        = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\r\n","\r\n","loss_object      = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction='auto')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gnm9bH1kviFG"},"source":["checkpoint_directory = base_path+\"/training_checkpoints/\"+str(l_rate)+\"_\"+str(dim_k)\r\n","SAVE_CKPT_FREQ = 5\r\n","\r\n","ckpt = tf.train.Checkpoint(step=tf.Variable(0), optimizer=optimizer, model=model)\r\n","manager = tf.train.CheckpointManager(ckpt, checkpoint_directory, max_to_keep=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4zWIz5Hvn7Z"},"source":["train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\r\n","val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\r\n","\r\n","train_score = F1Score(num_classes=max_answers, average='micro', name='train_score')\r\n","val_score = F1Score(num_classes=max_answers, average='micro', name='val_score')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dAWKkDQ3vvcX"},"source":["train_log_dir = base_path+'/logs/'+str(l_rate)+\"_\"+str(dim_k)+'/train'\r\n","val_log_dir   = base_path+'/logs/'+str(l_rate)+\"_\"+str(dim_k)+'/validation'\r\n","\r\n","train_summary_writer = tf.summary.create_file_writer(train_log_dir)\r\n","val_summary_writer = tf.summary.create_file_writer(val_log_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eROMc5zHvyaw"},"source":["\r\n","# @tf.function\r\n","def train_step(model, img, ques, ans, optimizer):\r\n","  with tf.GradientTape() as tape:\r\n","    # forward pass\r\n","    predictions = model([img, ques], training=True)\r\n","    loss = loss_object(ans, predictions)\r\n","\r\n","  # backward pass\r\n","  grads = tape.gradient(loss, model.trainable_variables)\r\n","  optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n","\r\n","  # record results\r\n","  train_loss(loss)\r\n","  train_score(ans, predictions)\r\n","\r\n","  # all gradients\r\n","  grads_ = list(zip(grads, model.trainable_variables))\r\n","  return grads_\r\n","\r\n","def test_step(model, img, ques, ans):\r\n","  predictions = model([img, ques])\r\n","  loss = loss_object(ans, predictions)\r\n","\r\n","  # record results\r\n","  val_loss(loss)\r\n","  val_score(ans, predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4y64KVyvzZE"},"source":["if manager.latest_checkpoint:\r\n","    ckpt.restore(manager.latest_checkpoint)\r\n","    print(\"Restored from {}\".format(manager.latest_checkpoint))\r\n","    START_EPOCH = int(manager.latest_checkpoint.split('-')[-1]) * SAVE_CKPT_FREQ\r\n","    print(\"Resume training from epoch: {}\".format(START_EPOCH))\r\n","else:\r\n","    print(\"Initializing from scratch\")\r\n","    START_EPOCH = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJ2Vpey6v9HN"},"source":["for epoch in range(START_EPOCH, EPOCHS):\r\n","\r\n","  start = time.time()\r\n","\r\n","  for img, ques, ans in (dataset_tr):\r\n","    grads = train_step(model, img, ques, ans, optimizer)\r\n","\r\n","  # tensorboard  \r\n","  with train_summary_writer.as_default():\r\n","    # Create a summary to monitor cost tensor\r\n","    tf.summary.scalar('loss', train_loss.result(), step=epoch)\r\n","    # Create a summary to monitor accuracy tensor\r\n","    tf.summary.scalar('f1_score', train_score.result(), step=epoch)\r\n","    # Create summaries to visualize weights\r\n","    for var in model.trainable_variables:\r\n","        tf.summary.histogram(var.name, var, step=epoch)\r\n","    # Summarize all gradients\r\n","    for grad, var in grads:\r\n","        tf.summary.histogram(var.name + '/gradient', grad, step=epoch)\r\n","\r\n","  for img, ques, ans in (dataset_vl):\r\n","    test_step(model, img, ques, ans)\r\n","  \r\n","  # tensorboard\r\n","  with val_summary_writer.as_default():\r\n","    # Create a summary to monitor cost tensor\r\n","    tf.summary.scalar('loss', val_loss.result(), step=epoch)\r\n","    # Create a summary to monitor accuracy tensor\r\n","    tf.summary.scalar('f1_score', val_score.result(), step=epoch)\r\n","  \r\n","  template = 'Epoch {}, loss: {:.4f}, f1_score: {:.4f}, val loss: {:.4f}, val f1_score: {:.4f}, time: {:.0f} sec'\r\n","  print (template.format(epoch + 1,\r\n","                         train_loss.result(), \r\n","                         train_score.result(),\r\n","                         val_loss.result(), \r\n","                         val_score.result(),\r\n","                         (time.time() - start)))\r\n","\r\n","  # Reset metrics every epoch\r\n","  train_loss.reset_states()\r\n","  train_score.reset_states()\r\n","  val_loss.reset_states()\r\n","  val_score.reset_states()\r\n","\r\n","  # save checkpoint every SAVE_CKPT_FREQ step\r\n","  ckpt.step.assign_add(1)\r\n","  if int(ckpt.step) % SAVE_CKPT_FREQ == 0:\r\n","      manager.save()\r\n","      print('Saved checkpoint.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SKlGwognSwzf","executionInfo":{"status":"ok","timestamp":1611950214668,"user_tz":-60,"elapsed":138103,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}},"outputId":"f72a44ec-bcdd-454b-d221-22e01140e959"},"source":["  pred = vqa_model.predict_generator(test_generator)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n","  warnings.warn('`Model.predict_generator` is deprecated and '\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"tcoedQR9Nsg6","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1611950214671,"user_tz":-60,"elapsed":137681,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}},"outputId":"3d724171-1c1f-4791-ac26-a6a8165c4823"},"source":["import os\r\n","from datetime import datetime\r\n","from google.colab import files\r\n","\r\n","def create_csv(results, results_dir='./'):\r\n","\r\n","    csv_fname = 'results_'\r\n","    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\r\n","\r\n","    with open(os.path.join(results_dir, csv_fname), 'w') as f:\r\n","\r\n","        f.write('Id,Category\\n')\r\n","\r\n","        for key, value in results.items():\r\n","            f.write(str(key) + ',' + str(value) + '\\n')\r\n","    \r\n","    return csv_fname\r\n","\r\n","results = {}\r\n","\r\n","for i in range(len(pred)):\r\n","    results[test_generator.list_IDs[i]] = np.argmax(pred[i])\r\n","\r\n","name = create_csv(results)\r\n","\r\n","files.download(name)"],"execution_count":29,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_d7f18e8e-820e-4b2e-b4c1-1c2d7ca09fee\", \"results_Jan29_19-56-53.csv\", 61350)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"SksWUrZuAnSF","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1612012569053,"user_tz":-60,"elapsed":639,"user":{"displayName":"Edoardo Putti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7oizfBqgR_sAV-NMtKUxel-6CGnUb_8hqENyzJA=s64","userId":"10533665133871308250"}},"outputId":"23635f60-598c-4a18-b09d-e326f008ca64"},"source":[""],"execution_count":149,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_1d13ca18-acbd-476d-9bc1-ac23299698e9\", \"\", 815104)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"d1TwE1nEbx79"},"source":[""],"execution_count":null,"outputs":[]}]}